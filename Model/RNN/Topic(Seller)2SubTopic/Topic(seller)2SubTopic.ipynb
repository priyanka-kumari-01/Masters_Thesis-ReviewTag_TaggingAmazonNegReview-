{"cells":[{"cell_type":"markdown","source":["#Amazon Review- Tagging Negative Review in Amazon Product Review with RNN Model"],"metadata":{"id":"ak_6DxSmfMze"},"id":"ak_6DxSmfMze"},{"cell_type":"markdown","source":["#Introduction\n"],"metadata":{"id":"bk83BnFefMu2"},"id":"bk83BnFefMu2"},{"cell_type":"markdown","source":["### Connecting to Golab Colab"],"metadata":{"id":"xXqsYn8kfJ1z"},"id":"xXqsYn8kfJ1z"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDFg8R5Kf8ks","outputId":"5e6ee3f1-4516-45b4-82ef-d7f137fa8279","executionInfo":{"status":"ok","timestamp":1681976958085,"user_tz":300,"elapsed":29239,"user":{"displayName":"AmazonReview Topic","userId":"03947028823040931104"}}},"id":"sDFg8R5Kf8ks","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Importing Libraries"],"metadata":{"id":"Ld6jyTiKe-cP"},"id":"Ld6jyTiKe-cP"},{"cell_type":"code","execution_count":null,"id":"f23a152d","metadata":{"id":"f23a152d"},"outputs":[],"source":["import functools\n","import sys\n","import csv\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchtext\n","import tqdm\n","import time\n"]},{"cell_type":"markdown","source":["### Dataset"],"metadata":{"id":"C74cluXle6p5"},"id":"C74cluXle6p5"},{"cell_type":"code","execution_count":null,"id":"7b34799a","metadata":{"id":"7b34799a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681977408271,"user_tz":300,"elapsed":442814,"user":{"displayName":"AmazonReview Topic","userId":"03947028823040931104"}},"outputId":"e7c97dde-e723-4b9d-8ce4-f2928a921f08"},"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/wiki.en.vec: 6.60GB [00:39, 167MB/s]                            \n","  0%|          | 0/2519370 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n","100%|██████████| 2519370/2519370 [06:00<00:00, 6994.06it/s]\n"]}],"source":["tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n","vectors = torchtext.vocab.FastText()\n"]},{"cell_type":"code","source":["df_train = \"/content/drive/MyDrive/Masters_Thesis/Dataset/encoded_data/encoded_topic(seller)2subTopic_train.csv\"\n","df_test = \"/content/drive/MyDrive/Masters_Thesis/Dataset/encoded_data/encoded_topic(seller)2subTopic_test.csv\"\n","target_list = 'encode_sub_seller_topic'\n","train_dataloader, valid_dataloader, vocab_size, pad_index, output_dim, vocab = get_data_loaders(train_dataframe= df_train,valid_dataframe=df_test,target_list=target_list)"],"metadata":{"id":"lcM3VZDf5YzP"},"id":"lcM3VZDf5YzP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Implementation"],"metadata":{"id":"vl_1CgP-euL9"},"id":"vl_1CgP-euL9"},{"cell_type":"code","source":["embedding_dim = 300\n","hidden_dim = 300\n","n_layers = 2\n","bidirectional = True\n","dropout_rate = 0.5\n","\n","model = RNN(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate,pad_index)\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTWy4Qvy4SIB","outputId":"c87cdf35-a6f0-49cc-f766-e62271f68fe4","executionInfo":{"status":"ok","timestamp":1681977652726,"user_tz":300,"elapsed":382,"user":{"displayName":"AmazonReview Topic","userId":"03947028823040931104"}}},"id":"UTWy4Qvy4SIB","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 1,685,102 trainable parameters\n"]}]},{"cell_type":"code","source":["model = apply_weights(model,vocab)\n","\n","lr = 5e-4\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","loss_function = nn.CrossEntropyLoss()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","loss_function = loss_function.to(device)"],"metadata":{"id":"WY_0dNjL4dFn"},"id":"WY_0dNjL4dFn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(total_epoch=20, model=model, train_loader=train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2L3k-cNK5Fgh","outputId":"c2718f1b-c092-4253-ed1e-28743817f717","executionInfo":{"status":"ok","timestamp":1681977706516,"user_tz":300,"elapsed":44037,"user":{"displayName":"AmazonReview Topic","userId":"03947028823040931104"}}},"id":"2L3k-cNK5Fgh","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Epoch  |  Train Loss  | Train Accuracy |  Elapsed \n","---------------------------------------------------\n","   1    |   0.498147   |    0.748252    |   0.06   \n","   2    |   0.257927   |    0.897891    |   0.04   \n","   3    |   0.228163   |    0.909291    |   0.05   \n","   4    |   0.162509   |    0.942681    |   0.03   \n","   5    |   0.275416   |    0.909646    |   0.03   \n","   6    |   0.160347   |    0.942963    |   0.03   \n","   7    |   0.087709   |    0.969970    |   0.03   \n","   8    |   0.137259   |    0.959754    |   0.03   \n","   9    |   0.119171   |    0.962476    |   0.04   \n","  10    |   0.047396   |    0.981671    |   0.04   \n","  11    |   0.052297   |    0.981370    |   0.03   \n","  12    |   0.057362   |    0.981707    |   0.03   \n","  13    |   0.045948   |    0.986797    |   0.03   \n","  14    |   0.060918   |    0.980469    |   0.03   \n","  15    |   0.035256   |    0.989501    |   0.03   \n","  16    |   0.052377   |    0.984994    |   0.05   \n","  17    |   0.062344   |    0.982272    |   0.03   \n","  18    |   0.040002   |    0.987380    |   0.03   \n","  19    |   0.038752   |    0.987098    |   0.03   \n","  20    |   0.032938   |    0.990084    |   0.03   \n"]}]},{"cell_type":"code","source":["predictions = valid(model, valid_dataloader) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5M1Leqw77yr7","outputId":"09cb1fd2-71d4-44b4-f6e9-7878a0353979","executionInfo":{"status":"ok","timestamp":1681977706516,"user_tz":300,"elapsed":6,"user":{"displayName":"AmazonReview Topic","userId":"03947028823040931104"}}},"id":"5M1Leqw77yr7","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Valid Loss  | Valid Accuracy |  Elapsed \n","-----------------------------------------\n","  0.109223   |    0.969952    |   0.00   \n"]}]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/Masters_Thesis/Models/RNN\"\n","saveModel(model=model, path=path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPEnp43Xfvne","executionInfo":{"status":"ok","timestamp":1681977709147,"user_tz":300,"elapsed":1406,"user":{"displayName":"AmazonReview Topic","userId":"03947028823040931104"}},"outputId":"c81c292f-7f0c-4afb-91ec-211ca5cccf64"},"id":"hPEnp43Xfvne","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All files saved\n"]}]},{"cell_type":"code","source":["saveCSVValidationResult(predictions,path)"],"metadata":{"id":"Gjn-gJsx8myz"},"id":"Gjn-gJsx8myz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Preprocessing & DataLoader"],"metadata":{"id":"0VXqfXj0eqFO"},"id":"0VXqfXj0eqFO"},{"cell_type":"code","execution_count":null,"id":"6fa2f2e3","metadata":{"id":"6fa2f2e3"},"outputs":[],"source":["def tokenize_data(text, tokenizer, max_length):\n","    tokens = tokenizer(text)[:max_length]\n","    length = len(tokens)\n","    return {'tokens': tokens, 'length': length}\n","\n","def numericalize_data(example, vocab):\n","    ids = [vocab[token] for token in example['tokens']]\n","    return {'ids': ids}\n","\n","def collate(batch, pad_index,target_list):\n","    batch_ids = [torch.tensor(i['ids']) for i in batch]  # Convert ids list to tensor\n","    batch_ids = pad_sequence(batch_ids, padding_value=torch.tensor(pad_index), batch_first=True)\n","    batch_length = [torch.tensor(i['length']) for i in batch]  # Convert length list to tensor\n","    batch_length = torch.stack(batch_length)\n","    batch_label = [torch.tensor(i[target_list]) for i in batch]  # Convert reviewText list to tensor\n","    batch_label = torch.stack(batch_label)\n","    batch_text = [(i['reviewText']) for i in batch]\n","    batch = {'ids': batch_ids,\n","             'length': batch_length,\n","             target_list: batch_label,\n","             'text': batch_text}\n","    return batch\n","\n","def get_data_loaders(train_dataframe, valid_dataframe,target_list, max_length=256,min_freq = 5, train_batch_size=16, learning_rate=2e-5):\n","    df_train = pd.read_csv(train_dataframe)\n","    df_test = pd.read_csv(valid_dataframe)\n","    df_train = pd.concat([df_train, df_train['reviewText'].apply(tokenize_data, tokenizer=tokenizer, max_length=max_length).apply(pd.Series)], axis=1)\n","    df_test = pd.concat([df_test, df_test['reviewText'].apply(tokenize_data, tokenizer=tokenizer, max_length=max_length).apply(pd.Series)], axis=1)\n","    special_tokens = ['<unk>', '<pad>']\n","    tokens = df_train['tokens'].tolist()\n","    vocab = torchtext.vocab.build_vocab_from_iterator(tokens, min_freq=min_freq, specials=special_tokens)\n","    unk_index = vocab['<unk>']\n","    pad_index = vocab['<pad>']\n","    vocab.set_default_index(unk_index)\n","    df_train = pd.concat([df_train, df_train.apply(numericalize_data, vocab=vocab, axis=1).apply(pd.Series)], axis=1)\n","    df_test = pd.concat([df_test, df_test.apply(numericalize_data, vocab=vocab, axis=1).apply(pd.Series)], axis=1)\n","    df_train = df_train[['ids', target_list, 'length','reviewText']]\n","    df_test = df_test[['ids', target_list, 'length','reviewText']]\n","    vocab_size = len(vocab)\n","    output_dim = df_train[target_list].nunique()\n","    train = df_train.apply(lambda row: {\n","        'ids': row['ids'],\n","        'length': row['length'],\n","        target_list: row[target_list],\n","        'reviewText': row['reviewText']\n","        }, axis=1).tolist()\n","    validation = df_test.apply(lambda row: {\n","        'ids': row['ids'],\n","        'length': row['length'],\n","        target_list: row[target_list],\n","        'reviewText': row['reviewText']\n","        }, axis=1).tolist()\n","    batch_size = 64\n","    collate_fn = functools.partial(collate, pad_index=pad_index, target_list=target_list)\n","    train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n","    valid_dataloader = DataLoader(validation, batch_size=batch_size, collate_fn=collate_fn)\n","\n","    return train_dataloader, valid_dataloader, vocab_size, pad_index, output_dim, vocab"]},{"cell_type":"markdown","source":["### RNN Model"],"metadata":{"id":"cqIFYbtOemiE"},"id":"cqIFYbtOemiE"},{"cell_type":"code","source":["class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional,\n","                 dropout_rate, pad_index):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,\n","                            dropout=dropout_rate, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        \n","    def forward(self, ids, length):\n","        # ids = [batch size, seq len]\n","        # length = [batch size]\n","        embedded = self.dropout(self.embedding(ids))\n","        # embedded = [batch size, seq len, embedding dim]\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True, \n","                                                            enforce_sorted=False)\n","        _, hidden = self.rnn(packed_embedded)\n","        # hidden = [n layers * n directions, batch size, hidden dim]\n","        # cell = [n layers * n directions, batch size, hidden dim]\n","        if self.rnn.bidirectional:\n","            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n","            # hidden = [batch size, hidden dim * 2]\n","        else:\n","            hidden = self.dropout(hidden[-1])\n","            # hidden = [batch size, hidden dim]\n","        prediction = self.fc(hidden)\n","        # prediction = [batch size, output dim]\n","        return prediction"],"metadata":{"id":"PZmIPVWpdvDC"},"id":"PZmIPVWpdvDC","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"1f3a3894","metadata":{"id":"1f3a3894"},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def initialize_weights(m):\n","    if isinstance(m, nn.Linear):\n","        nn.init.xavier_normal_(m.weight)\n","        nn.init.zeros_(m.bias)\n","    elif isinstance(m, nn.LSTM):\n","        for name, param in m.named_parameters():\n","            if 'bias' in name:\n","                nn.init.zeros_(param)\n","            elif 'weight' in name:\n","                nn.init.orthogonal_(param)\n","\n","def apply_weights(model,vocab):\n","  model.apply(initialize_weights)\n","  pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n","  model.embedding.weight.data = pretrained_embedding\n","  return model"]},{"cell_type":"markdown","source":["### Training & Validation"],"metadata":{"id":"qMN49gDCeipD"},"id":"qMN49gDCeipD"},{"cell_type":"code","source":["def train(total_epoch, model, train_loader):\n","    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Accuracy':^12} | {'Elapsed':^9}\")\n","    print(\"-\"*51)\n","    for epoch in range(total_epoch):\n","      t0_epoch = time.time() \n","      tr_loss = 0\n","      n_correct = 0\n","      nb_tr_steps = 0\n","      nb_tr_examples = 0\n","      model.train()\n","      for _,data in enumerate(train_loader, 0):\n","      # for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n","        input_ids  = data['ids'].to(device)\n","        length = data['length']\n","        targets  = data[target_list].to(device)\n","        # print(input_ids)\n","        outputs = model(input_ids,length)\n","\n","        loss = loss_function(outputs, targets)\n","        tr_loss += loss.item()\n","        n_correct += get_accuracy(outputs, targets).item()\n","        nb_tr_steps += 1\n","        nb_tr_examples+=targets.size(0)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","      time_elapsed = (time.time() - t0_epoch)/60\n","      epoch_loss = tr_loss/nb_tr_steps\n","      epoch_accu = n_correct/nb_tr_steps # np.mean(n_correct)\n","      print(f\"{epoch + 1:^7} | {epoch_loss:^12.6f} | {epoch_accu:^14.6f} | {time_elapsed:^9.2f}\")\n","\n","def valid(model, testing_loader):\n","    print(f\"{'Valid Loss':^12} | {'Valid Accuracy':^12} | {'Elapsed':^9}\")\n","    print(\"-\"*41)\n","    model.eval()\n","    t0_epoch = time.time() \n","    tr_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    predictions = []\n","    with torch.no_grad():\n","        for _, data in enumerate(testing_loader, 0):\n","        #   input_ids, targets, text = tuple(t.to(device) for t in data)\n","          input_ids  = data['ids'].to(device)\n","          length = data['length']\n","          targets  = data[target_list].to(device)\n","          text = data['text']\n","          outputs = model(input_ids,length)\n","          \n","\n","          loss = loss_function(outputs, targets)\n","          tr_loss += loss.item()\n","          n_correct += get_accuracy(outputs, targets).item()\n","\n","          nb_tr_steps += 1\n","          nb_tr_examples+=targets.size(0)\n","\n","          #Todo: get text\n","          for i in range(len(text)):\n","            predictions.append({\n","                'text': text[i],\n","                'predicted': switch_issue(outputs.argmax(dim=-1)[i].item()),\n","                'target': switch_issue(targets[i].item())\n","                })\n","\n","    time_elapsed = (time.time() - t0_epoch)/60                \n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = n_correct/nb_tr_steps # np.mean(n_correct)\n","    print(f\"{epoch_loss:^12.6f} | {epoch_accu:^14.6f} | {time_elapsed:^9.2f}\")\n","    return predictions\n","\n","def switch_issue(issue_type):\n","    switcher = {\n","      1: 'Delivery and Return Issue',\n","      0: 'Product Authenticity Issue'\n","      }\n","    return switcher.get(issue_type, \"Invalid Issue Type\")\n","\n","def get_accuracy(prediction, label):\n","    batch_size, _ = prediction.shape\n","    predicted_classes = prediction.argmax(dim=-1)\n","    correct_predictions = predicted_classes.eq(label).sum()\n","    accuracy = correct_predictions / batch_size\n","    return accuracy\n"],"metadata":{"id":"x8IXQ2bT4uPZ"},"id":"x8IXQ2bT4uPZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save Files"],"metadata":{"id":"_nv1o7nKB4iI"},"id":"_nv1o7nKB4iI"},{"cell_type":"code","source":["def saveCSVValidationResult(predictions, path, fileName=\"/Topic(Seller)2SubTopic/back_only_01/result.csv\"):\n","    path = path+\"/\"+fileName\n","    with open(path, mode='w', newline='', encoding='utf-8') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Text', 'Predicted', 'Target'])\n","        for example in predictions:\n","            writer.writerow([example['text'], example['predicted'], example['target']])"],"metadata":{"id":"HOrTHj2TemRg"},"execution_count":null,"outputs":[],"id":"HOrTHj2TemRg"},{"cell_type":"code","source":["def saveModel(model, path):\n","  MODEL_PATH = path+'/Topic(Seller)2SubTopic/back_only_01/model.pth'\n","  torch.save(model.state_dict(), MODEL_PATH)  \n","  print('All files saved')\n","\n","def loadModel(path):\n","  MODEL_PATH = path+'/model.pth'\n","  model.load_state_dict(torch.load(MODEL_PATH))\n","  return model, tokenizer"],"metadata":{"id":"5w-rUwH1crVv"},"execution_count":null,"outputs":[],"id":"5w-rUwH1crVv"},{"cell_type":"code","source":[],"metadata":{"id":"qzvY5Yw5s9dP"},"execution_count":null,"outputs":[],"id":"qzvY5Yw5s9dP"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[],"collapsed_sections":["xXqsYn8kfJ1z","Ld6jyTiKe-cP","C74cluXle6p5","0VXqfXj0eqFO"]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}