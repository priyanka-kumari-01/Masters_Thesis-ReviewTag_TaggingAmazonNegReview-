{"cells":[{"cell_type":"markdown","source":["#Amazon Review- Tagging Negative Review in Amazon Product Review with CNN Model"],"metadata":{"id":"ak_6DxSmfMze"},"id":"ak_6DxSmfMze"},{"cell_type":"markdown","source":["#Introduction\n"],"metadata":{"id":"bk83BnFefMu2"},"id":"bk83BnFefMu2"},{"cell_type":"markdown","source":["### Connecting to Golab Colab"],"metadata":{"id":"xXqsYn8kfJ1z"},"id":"xXqsYn8kfJ1z"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDFg8R5Kf8ks","outputId":"f4eb59fd-ef4e-4017-9ca4-ac8e1d56a9a9","executionInfo":{"status":"ok","timestamp":1681588530164,"user_tz":300,"elapsed":20554,"user":{"displayName":"Priyanka Kumari","userId":"16630163271427248504"}}},"id":"sDFg8R5Kf8ks","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Importing Libraries"],"metadata":{"id":"Ld6jyTiKe-cP"},"id":"Ld6jyTiKe-cP"},{"cell_type":"code","execution_count":null,"id":"f23a152d","metadata":{"id":"f23a152d"},"outputs":[],"source":["import functools\n","import sys\n","import csv\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchtext\n","import tqdm\n","import time\n"]},{"cell_type":"markdown","source":["### Dataset"],"metadata":{"id":"C74cluXle6p5"},"id":"C74cluXle6p5"},{"cell_type":"code","execution_count":null,"id":"7b34799a","metadata":{"id":"7b34799a"},"outputs":[],"source":["tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n","vectors = torchtext.vocab.FastText()"]},{"cell_type":"code","source":["df_train = '/content/drive/MyDrive/Masters_Thesis/Dataset/topicReviewText/topicReviewText_train.csv'\n","df_test = '/content/drive/MyDrive/Masters_Thesis/Dataset/topicReviewText/topicReviewText_test.csv'\n","target_list = 'encoded_sub_topic'\n","train_dataloader, valid_dataloader, vocab_size, pad_index, output_dim, vocab = get_data_loaders(train_dataframe= df_train,valid_dataframe=df_test,target_list=target_list)"],"metadata":{"id":"lcM3VZDf5YzP"},"id":"lcM3VZDf5YzP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv(df_train)\n","df_test = pd.read_csv(df_test)"],"metadata":{"id":"t8Q5PWbxo6fR"},"id":"t8Q5PWbxo6fR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['encoded_sub_topic'].unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AG8VhMEZqNJJ","executionInfo":{"status":"ok","timestamp":1681591499608,"user_tz":300,"elapsed":5,"user":{"displayName":"Priyanka Kumari","userId":"16630163271427248504"}},"outputId":"42830946-c1c1-4c22-9af4-95ff2be6a847"},"id":"AG8VhMEZqNJJ","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3, 4])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["df_test['encoded_sub_topic'].unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WGYXRZqqKy6","executionInfo":{"status":"ok","timestamp":1681591483356,"user_tz":300,"elapsed":7,"user":{"displayName":"Priyanka Kumari","userId":"16630163271427248504"}},"outputId":"7f2979c9-f6a8-4146-adf4-c344101cfe33"},"id":"5WGYXRZqqKy6","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3, 4])"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["### Model Implementation"],"metadata":{"id":"vl_1CgP-euL9"},"id":"vl_1CgP-euL9"},{"cell_type":"code","source":["embedding_dim = 300\n","hidden_dim = 300\n","n_layers = 2\n","bidirectional = True\n","dropout_rate = 0.5\n","\n","model = CNN(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, \n","             pad_index)\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTWy4Qvy4SIB","outputId":"4350cf15-0ed4-4cb8-8c1c-512595ea50c9","executionInfo":{"status":"ok","timestamp":1681591224831,"user_tz":300,"elapsed":7,"user":{"displayName":"Priyanka Kumari","userId":"16630163271427248504"}}},"id":"UTWy4Qvy4SIB","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 1,792,805 trainable parameters\n"]}]},{"cell_type":"code","source":["model = apply_weights(model,vocab)\n","\n","lr = 5e-4\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","loss_function = nn.CrossEntropyLoss()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","loss_function = loss_function.to(device)\n"],"metadata":{"id":"WY_0dNjL4dFn"},"id":"WY_0dNjL4dFn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(total_epoch=20, model=model, train_loader=train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2L3k-cNK5Fgh","outputId":"4622dcba-0bee-4bf9-bd76-247ece8c483b","executionInfo":{"status":"ok","timestamp":1681591292015,"user_tz":300,"elapsed":62175,"user":{"displayName":"Priyanka Kumari","userId":"16630163271427248504"}}},"id":"2L3k-cNK5Fgh","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Epoch  |  Train Loss  | Train Accuracy |  Elapsed \n","---------------------------------------------------\n","   1    |   0.590720   |    0.783408    |   0.05   \n","   2    |   0.247831   |    0.913592    |   0.05   \n","   3    |   0.195833   |    0.933100    |   0.06   \n","   4    |   0.160178   |    0.945202    |   0.05   \n","   5    |   0.134076   |    0.952548    |   0.05   \n","   6    |   0.109865   |    0.963680    |   0.05   \n","   7    |   0.090037   |    0.971904    |   0.05   \n","   8    |   0.079504   |    0.977698    |   0.05   \n","   9    |   0.062796   |    0.982039    |   0.05   \n","  10    |   0.053769   |    0.984681    |   0.05   \n","  11    |   0.047274   |    0.986134    |   0.06   \n","  12    |   0.042154   |    0.988509    |   0.05   \n","  13    |   0.036837   |    0.990791    |   0.05   \n","  14    |   0.035228   |    0.990584    |   0.05   \n","  15    |   0.032059   |    0.992964    |   0.05   \n","  16    |   0.029113   |    0.993067    |   0.05   \n","  17    |   0.027880   |    0.992599    |   0.05   \n","  18    |   0.024401   |    0.993791    |   0.05   \n","  19    |   0.027907   |    0.993067    |   0.05   \n","  20    |   0.024352   |    0.994102    |   0.05   \n"]}]},{"cell_type":"code","source":["predictions = valid(model, valid_dataloader) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5M1Leqw77yr7","outputId":"07a088ca-d884-4903-d8ad-e813e4cd63e0","executionInfo":{"status":"ok","timestamp":1681591318842,"user_tz":300,"elapsed":804,"user":{"displayName":"Priyanka Kumari","userId":"16630163271427248504"}}},"id":"5M1Leqw77yr7","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Train Loss  | Train Accuracy |  Elapsed \n","-----------------------------------------\n"," 13.820806   |    0.011102    |   0.01   \n"]}]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/Masters_Thesis/Models/NewModel\"\n","saveModel(model=model, path=path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPEnp43Xfvne","executionInfo":{"status":"ok","timestamp":1681591159527,"user_tz":300,"elapsed":304,"user":{"displayName":"Priyanka Kumari","userId":"16630163271427248504"}},"outputId":"315ed0e5-40b6-42f9-c53c-e4b62f94de09"},"id":"hPEnp43Xfvne","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All files saved\n"]}]},{"cell_type":"code","source":["saveCSVValidationResult(predictions,path)"],"metadata":{"id":"Gjn-gJsx8myz"},"id":"Gjn-gJsx8myz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Preprocessing & DataLoader"],"metadata":{"id":"0VXqfXj0eqFO"},"id":"0VXqfXj0eqFO"},{"cell_type":"code","execution_count":null,"id":"6fa2f2e3","metadata":{"id":"6fa2f2e3"},"outputs":[],"source":["def tokenize_data(text, tokenizer, max_length):\n","    tokens = tokenizer(text)[:max_length]\n","    length = len(tokens)\n","    return {'tokens': tokens, 'length': length}\n","\n","def numericalize_data(example, vocab):\n","    ids = [vocab[token] for token in example['tokens']]\n","    return {'ids': ids}\n","\n","def collate(batch, pad_index,target_list):\n","    batch_ids = [torch.tensor(i['ids']) for i in batch]  # Convert ids list to tensor\n","    batch_ids = pad_sequence(batch_ids, padding_value=torch.tensor(pad_index), batch_first=True)\n","    batch_length = [torch.tensor(i['length']) for i in batch]  # Convert length list to tensor\n","    batch_length = torch.stack(batch_length)\n","    batch_label = [torch.tensor(i[target_list]) for i in batch]  # Convert reviewText list to tensor\n","    batch_label = torch.stack(batch_label)\n","    batch_text = [(i['reviewText']) for i in batch]\n","    batch = {'ids': batch_ids,\n","             'length': batch_length,\n","             target_list: batch_label,\n","             'text': batch_text}\n","    return batch\n","\n","def get_data_loaders(train_dataframe, valid_dataframe,target_list, max_length=256,min_freq = 5, train_batch_size=16, learning_rate=2e-5):\n","    df_train = pd.read_csv(train_dataframe)\n","    df_test = pd.read_csv(valid_dataframe)\n","    df_train = pd.concat([df_train, df_train['reviewText'].apply(tokenize_data, tokenizer=tokenizer, max_length=max_length).apply(pd.Series)], axis=1)\n","    df_test = pd.concat([df_test, df_test['reviewText'].apply(tokenize_data, tokenizer=tokenizer, max_length=max_length).apply(pd.Series)], axis=1)\n","    special_tokens = ['<unk>', '<pad>']\n","    tokens = df_train['tokens'].tolist()\n","    vocab = torchtext.vocab.build_vocab_from_iterator(tokens, min_freq=min_freq, specials=special_tokens)\n","    unk_index = vocab['<unk>']\n","    pad_index = vocab['<pad>']\n","    vocab.set_default_index(unk_index)\n","    df_train = pd.concat([df_train, df_train.apply(numericalize_data, vocab=vocab, axis=1).apply(pd.Series)], axis=1)\n","    df_test = pd.concat([df_test, df_test.apply(numericalize_data, vocab=vocab, axis=1).apply(pd.Series)], axis=1)\n","    df_train = df_train[['ids', target_list, 'length','reviewText']]\n","    df_test = df_test[['ids', target_list, 'length','reviewText']]\n","    vocab_size = len(vocab)\n","    output_dim = df_train[target_list].nunique()\n","    train = df_train.apply(lambda row: {\n","        'ids': row['ids'],\n","        'length': row['length'],\n","        target_list: row[target_list],\n","        'reviewText': row['reviewText']\n","        }, axis=1).tolist()\n","    validation = df_test.apply(lambda row: {\n","        'ids': row['ids'],\n","        'length': row['length'],\n","        target_list: row[target_list],\n","        'reviewText': row['reviewText']\n","        }, axis=1).tolist()\n","    batch_size = 64\n","    collate_fn = functools.partial(collate, pad_index=pad_index, target_list=target_list)\n","    train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n","    valid_dataloader = DataLoader(validation, batch_size=batch_size, collate_fn=collate_fn)\n","\n","    return train_dataloader, valid_dataloader, vocab_size, pad_index, output_dim, vocab"]},{"cell_type":"markdown","source":["### CNN Model"],"metadata":{"id":"cqIFYbtOemiE"},"id":"cqIFYbtOemiE"},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, \n","                 dropout_rate, pad_index, filter_sizes=[3, 4, 5], num_filters=[100, 100, 100]):\n","\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n","        \n","        self.conv1d_list = nn.ModuleList([\n","            nn.Conv1d(in_channels=embedding_dim,\n","                      out_channels=num_filters[i],\n","                      kernel_size=filter_sizes[i])\n","            for i in range(len(filter_sizes))\n","        ])\n","        self.fc = nn.Linear(sum(num_filters), output_dim)  # modify the input dimension of the linear layer\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.flatten = nn.Flatten()  # add a flatten layer\n","        \n","    def forward(self, ids, length):\n","        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n","        x_embed = self.embedding(ids).float()\n","\n","        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n","        # Output shape: (b, embed_dim, max_len)\n","        x_reshaped = x_embed.permute(0, 2, 1)\n","\n","        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n","        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n","\n","        # Max pooling. Output shape: (b, num_filters[i], 1)\n","        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n","            for x_conv in x_conv_list]\n","        \n","        # Concatenate x_pool_list to feed the fully connected layer.\n","        # Output shape: (b, sum(num_filters))\n","        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n","                         dim=1)\n","        \n","        x_fc = self.flatten(x_fc)  \n","        \n","        # Compute logits. Output shape: (b, n_classes)\n","        prediction = self.fc(self.dropout(x_fc))\n","        return prediction"],"metadata":{"id":"PZmIPVWpdvDC"},"id":"PZmIPVWpdvDC","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"1f3a3894","metadata":{"id":"1f3a3894"},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def initialize_weights(m):\n","    if isinstance(m, nn.Linear):\n","        nn.init.xavier_normal_(m.weight)\n","        nn.init.zeros_(m.bias)\n","    elif isinstance(m, nn.LSTM):\n","        for name, param in m.named_parameters():\n","            if 'bias' in name:\n","                nn.init.zeros_(param)\n","            elif 'weight' in name:\n","                nn.init.orthogonal_(param)\n","\n","def apply_weights(model,vocab):\n","  model.apply(initialize_weights)\n","  pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n","  model.embedding.weight.data = pretrained_embedding\n","  return model"]},{"cell_type":"markdown","source":["### Training & Validation"],"metadata":{"id":"qMN49gDCeipD"},"id":"qMN49gDCeipD"},{"cell_type":"code","source":["def train(total_epoch, model, train_loader):\n","    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Accuracy':^12} | {'Elapsed':^9}\")\n","    print(\"-\"*51)\n","    for epoch in range(total_epoch):\n","      t0_epoch = time.time() \n","      tr_loss = 0\n","      n_correct = 0\n","      nb_tr_steps = 0\n","      nb_tr_examples = 0\n","      model.train()\n","      for _,data in enumerate(train_loader, 0):\n","      # for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n","        input_ids  = data['ids'].to(device)\n","        length = data['length']\n","        targets  = data[target_list].to(device)\n","        # print(input_ids)\n","        outputs = model(input_ids,length)\n","\n","        loss = loss_function(outputs, targets)\n","        tr_loss += loss.item()\n","        n_correct += get_accuracy(outputs, targets).item()\n","        nb_tr_steps += 1\n","        nb_tr_examples+=targets.size(0)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","      time_elapsed = (time.time() - t0_epoch)/60\n","      epoch_loss = tr_loss/nb_tr_steps\n","      epoch_accu = n_correct/nb_tr_steps # np.mean(n_correct)\n","      print(f\"{epoch + 1:^7} | {epoch_loss:^12.6f} | {epoch_accu:^14.6f} | {time_elapsed:^9.2f}\")\n","\n","def valid(model, testing_loader):\n","    print(f\"{'Train Loss':^12} | {'Train Accuracy':^12} | {'Elapsed':^9}\")\n","    print(\"-\"*41)\n","    model.eval()\n","    t0_epoch = time.time() \n","    tr_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    predictions = []\n","    with torch.no_grad():\n","        for _, data in enumerate(testing_loader, 0):\n","        #   input_ids, targets, text = tuple(t.to(device) for t in data)\n","          input_ids  = data['ids'].to(device)\n","          length = data['length']\n","          targets  = data[target_list].to(device)\n","          text = data['text']\n","          outputs = model(input_ids,length)\n","          \n","\n","          loss = loss_function(outputs, targets)\n","          tr_loss += loss.item()\n","          n_correct += get_accuracy(outputs, targets).item()\n","\n","          nb_tr_steps += 1\n","          nb_tr_examples+=targets.size(0)\n","\n","          #Todo: get text\n","          for i in range(len(text)):\n","            predictions.append({\n","                'text': text[i],\n","                'predicted': switch_issue(outputs.argmax(dim=-1)[i].item()),\n","                'target': switch_issue(targets[i].item())\n","                })\n","\n","    time_elapsed = (time.time() - t0_epoch)/60                \n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = n_correct/nb_tr_steps # np.mean(n_correct)\n","    print(f\"{epoch_loss:^12.6f} | {epoch_accu:^14.6f} | {time_elapsed:^9.2f}\")\n","    return predictions\n","\n","def switch_issue(issue_type):\n","    switcher = {\n","      4: 'Product Description Issue',\n","      3: 'Delivery and Return Issue',\n","      2: 'Design Issue',\n","      1: 'Quality Issue',\n","      0: 'Product Authenticity Issue'\n","      }\n","    return switcher.get(issue_type, \"Invalid Issue Type\")\n","\n","def get_accuracy(prediction, label):\n","    batch_size, _ = prediction.shape\n","    predicted_classes = prediction.argmax(dim=-1)\n","    correct_predictions = predicted_classes.eq(label).sum()\n","    accuracy = correct_predictions / batch_size\n","    return accuracy\n"],"metadata":{"id":"x8IXQ2bT4uPZ"},"id":"x8IXQ2bT4uPZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save Files"],"metadata":{"id":"_nv1o7nKB4iI"},"id":"_nv1o7nKB4iI"},{"cell_type":"code","source":["def saveCSVValidationResult(predictions, path, fileName=\"/back_only_01/result.csv\"):\n","    path = path+\"/\"+fileName\n","    with open(path, mode='w', newline='', encoding='utf-8') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Text', 'Predicted', 'Target'])\n","        for example in predictions:\n","            writer.writerow([example['text'], example['predicted'], example['target']])"],"metadata":{"id":"HOrTHj2TemRg"},"execution_count":null,"outputs":[],"id":"HOrTHj2TemRg"},{"cell_type":"code","source":["def saveModel(model, path):\n","  MODEL_PATH = path+'/back_only_01/model.pth'\n","  torch.save(model.state_dict(), MODEL_PATH)  \n","  print('All files saved')\n","\n","def loadModel(path):\n","  MODEL_PATH = path+'/model.pth'\n","  model.load_state_dict(torch.load(MODEL_PATH))\n","  return model, tokenizer"],"metadata":{"id":"5w-rUwH1crVv"},"execution_count":null,"outputs":[],"id":"5w-rUwH1crVv"},{"cell_type":"code","source":[],"metadata":{"id":"qzvY5Yw5s9dP"},"execution_count":null,"outputs":[],"id":"qzvY5Yw5s9dP"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[],"collapsed_sections":["vl_1CgP-euL9","0VXqfXj0eqFO","cqIFYbtOemiE","qMN49gDCeipD"]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}